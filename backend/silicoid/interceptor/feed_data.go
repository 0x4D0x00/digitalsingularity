package interceptor

import (
	"context"
	"fmt"
	"time"

	"github.com/google/uuid"
)

// ContentChunk è¡¨ç¤ºå†…å®¹åˆ†å—
type ContentChunk struct {
	Index    int    `json:"index"`
	Total    int    `json:"total"`
	Content  string `json:"content"`
	IsLast   bool   `json:"is_last"`
	ToolName string `json:"tool_name"`
}

// chunkContent å°†å¤§å†…å®¹åˆ†å—å¤„ç†
func chunkContent(content string, toolName string, maxChunkSize int) []ContentChunk {
	if len(content) <= maxChunkSize {
		// å†…å®¹ä¸å¤§ï¼Œç›´æ¥è¿”å›
		return []ContentChunk{
			{
				Index:    1,
				Total:    1,
				Content:  content,
				IsLast:   true,
				ToolName: toolName,
			},
		}
	}
	
	// è®¡ç®—åˆ†å—æ•°é‡
	totalChunks := (len(content) + maxChunkSize - 1) / maxChunkSize
	chunks := make([]ContentChunk, 0, totalChunks)
	
	for i := 0; i < totalChunks; i++ {
		start := i * maxChunkSize
		end := start + maxChunkSize
		if end > len(content) {
			end = len(content)
		}
		
		chunk := ContentChunk{
			Index:    i + 1,
			Total:    totalChunks,
			Content:  content[start:end],
			IsLast:   i == totalChunks-1,
			ToolName: toolName,
		}
		chunks = append(chunks, chunk)
	}
	
	return chunks
}

// isLargeContent åˆ¤æ–­å†…å®¹æ˜¯å¦è¿‡å¤§éœ€è¦åˆ†æ‰¹å¤„ç†
func isLargeContent(content string, threshold int) bool {
	return len(content) > threshold
}

// BatchFeedResult åˆ†æ‰¹æŠ•å–‚ç»“æœ
type BatchFeedResult struct {
	Success     bool
	ChunkIndex  int
	Error       error
	RetryCount  int
}

// feedBatchWithRetry å¸¦é‡è¯•æœºåˆ¶çš„åˆ†æ‰¹æŠ•å–‚
func (s *SilicoIDInterceptor) feedBatchWithRetry(ctx context.Context, chunk ContentChunk, requestID string, data map[string]interface{}, maxRetries int) *BatchFeedResult {
	for attempt := 1; attempt <= maxRetries; attempt++ {
		logger.Printf("[%s] ğŸ“¤ å°è¯•æŠ•å–‚ç¬¬ %d/%d æ‰¹æ¬¡ (å°è¯• %d/%d)", requestID, chunk.Index, chunk.Total, attempt, maxRetries)
		
		err := s.feedSingleBatch(ctx, chunk, requestID, data)
		if err == nil {
			logger.Printf("[%s] âœ… ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚æˆåŠŸ", requestID, chunk.Index)
			return &BatchFeedResult{
				Success:    true,
				ChunkIndex: chunk.Index,
				RetryCount: attempt - 1,
			}
		}
		
		logger.Printf("[%s] âŒ ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚å¤±è´¥ (å°è¯• %d/%d): %v", requestID, chunk.Index, attempt, maxRetries, err)
		
		if attempt < maxRetries {
			// æŒ‡æ•°é€€é¿é‡è¯•
			backoffTime := time.Duration(attempt) * time.Second
			logger.Printf("[%s] â³ ç­‰å¾… %v åé‡è¯•", requestID, backoffTime)
			time.Sleep(backoffTime)
		}
	}
	
	return &BatchFeedResult{
		Success:    false,
		ChunkIndex: chunk.Index,
		Error:      fmt.Errorf("ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡", chunk.Index, maxRetries),
		RetryCount: maxRetries,
	}
}

// feedSingleBatch æŠ•å–‚å•ä¸ªæ•°æ®å—
func (s *SilicoIDInterceptor) feedSingleBatch(ctx context.Context, chunk ContentChunk, requestID string, data map[string]interface{}) error {
	// æ„é€ åˆ†æ‰¹æ¶ˆæ¯
	var batchMessage string
	if chunk.Total == 1 {
		// åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ­£å¸¸å¤„ç†
		batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœï¼š\n\n```json\n%s\n```\n\nè¯·åŸºäºä»¥ä¸Šç»“æœç»§ç»­å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚", 
			chunk.ToolName, chunk.Content)
	} else {
		// å¤šä¸ªæ‰¹æ¬¡ï¼Œæ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
		if chunk.IsLast {
			batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœ (ç¬¬ %d/%d æ‰¹æ¬¡ï¼Œæœ€åä¸€æ‰¹)ï¼š\n\n```json\n%s\n```\n\næ‰€æœ‰æ•°æ®å·²æŠ•å–‚å®Œæ¯•ï¼Œè¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ç»“æœç»§ç»­å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚", 
				chunk.ToolName, chunk.Index, chunk.Total, chunk.Content)
		} else {
			batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœ (ç¬¬ %d/%d æ‰¹æ¬¡)ï¼š\n\n```json\n%s\n```\n\nè¿™æ˜¯ç¬¬ %d æ‰¹æ•°æ®ï¼Œè¯·ç­‰å¾…æ‰€æœ‰æ•°æ®æŠ•å–‚å®Œæ¯•åå†å›ç­”ã€‚", 
				chunk.ToolName, chunk.Index, chunk.Total, chunk.Content, chunk.Index)
		}
	}
	
	// è·å–å½“å‰messages
	messages, _ := data["messages"].([]interface{})
	
	// æ·»åŠ å·¥å…·ç»“æœ
	messages = append(messages, map[string]interface{}{
		"id":      uuid.New().String(),
		"role":    "user",
		"content": batchMessage,
	})
	
	// æ›´æ–°è¯·æ±‚æ•°æ®
	data["messages"] = messages
	
	// å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œè®©AIç¡®è®¤æ¥æ”¶
	if !chunk.IsLast {
		logger.Printf("[%s] â³ ç¬¬ %d æ‰¹æ¬¡å·²æŠ•å–‚ï¼Œç­‰å¾…AIç¡®è®¤æ¥æ”¶", requestID, chunk.Index)
		
		// è°ƒç”¨AIç¡®è®¤æ¥æ”¶
		confirmResponse := s.openaiService.CreateChatCompletionNonStream(ctx, data)
		
		// æ£€æŸ¥ç¡®è®¤å“åº”æ˜¯å¦æœ‰é”™è¯¯
		if errObj, exists := confirmResponse["error"]; exists {
			return fmt.Errorf("AIç¡®è®¤è°ƒç”¨è¿”å›é”™è¯¯: %v", errObj)
		}
		
		// æå–ç¡®è®¤å“åº”
		if confirmChoices, ok := confirmResponse["choices"].([]interface{}); ok && len(confirmChoices) > 0 {
			if confirmChoice, ok := confirmChoices[0].(map[string]interface{}); ok {
				if confirmMessage, ok := confirmChoice["message"].(map[string]interface{}); ok {
					if confirmContent, ok := confirmMessage["content"].(string); ok {
						logger.Printf("[%s] âœ… ç¬¬ %d æ‰¹æ¬¡ç¡®è®¤å“åº”: %s", requestID, chunk.Index, truncateString(confirmContent, 100))
						
						// æ·»åŠ AIçš„ç¡®è®¤å“åº”
						messages = append(messages, map[string]interface{}{
							"id":      uuid.New().String(),
							"role":    "assistant",
							"content": confirmContent,
						})
						
						// æ›´æ–°messages
						data["messages"] = messages
					} else {
						return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”å†…å®¹")
					}
				} else {
					return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”æ¶ˆæ¯")
				}
			} else {
				return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”é€‰æ‹©")
			}
		} else {
			return fmt.Errorf("AIç¡®è®¤å“åº”æ ¼å¼æ— æ•ˆ")
		}
	}
	
	return nil
}

// validateAllBatchesFed éªŒè¯æ‰€æœ‰æ•°æ®å—æ˜¯å¦éƒ½å·²æŠ•å–‚
func (s *SilicoIDInterceptor) validateAllBatchesFed(feedResults []*BatchFeedResult, requestID string) error {
	successCount := 0
	failedBatches := []int{}
	
	for _, result := range feedResults {
		if result.Success {
			successCount++
		} else {
			failedBatches = append(failedBatches, result.ChunkIndex)
		}
	}
	
	totalBatches := len(feedResults)
	
	if successCount != totalBatches {
		logger.Printf("[%s] âŒ æ•°æ®å—æŠ•å–‚ä¸å®Œæ•´ï¼šæˆåŠŸ %d/%dï¼Œå¤±è´¥æ‰¹æ¬¡: %v", requestID, successCount, totalBatches, failedBatches)
		return fmt.Errorf("æ•°æ®å—æŠ•å–‚ä¸å®Œæ•´ï¼šæˆåŠŸ %d/%dï¼Œå¤±è´¥æ‰¹æ¬¡: %v", successCount, totalBatches, failedBatches)
	}
	
	logger.Printf("[%s] âœ… æ‰€æœ‰ %d ä¸ªæ•°æ®å—å·²æˆåŠŸæŠ•å–‚", requestID, totalBatches)
	return nil
}

// feedBatchWithRetryClaude å¸¦é‡è¯•æœºåˆ¶çš„åˆ†æ‰¹æŠ•å–‚ï¼ˆClaudeç‰ˆæœ¬ï¼‰
func (s *SilicoIDInterceptor) feedBatchWithRetryClaude(ctx context.Context, chunk ContentChunk, requestID string, data map[string]interface{}, maxRetries int) *BatchFeedResult {
	for attempt := 1; attempt <= maxRetries; attempt++ {
		logger.Printf("[%s] ğŸ“¤ å°è¯•æŠ•å–‚ç¬¬ %d/%d æ‰¹æ¬¡ (å°è¯• %d/%d) [Claude]", requestID, chunk.Index, chunk.Total, attempt, maxRetries)
		
		err := s.feedSingleBatchClaude(ctx, chunk, requestID, data)
		if err == nil {
			logger.Printf("[%s] âœ… ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚æˆåŠŸ [Claude]", requestID, chunk.Index)
			return &BatchFeedResult{
				Success:    true,
				ChunkIndex: chunk.Index,
				RetryCount: attempt - 1,
			}
		}
		
		logger.Printf("[%s] âŒ ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚å¤±è´¥ (å°è¯• %d/%d): %v [Claude]", requestID, chunk.Index, attempt, maxRetries, err)
		
		if attempt < maxRetries {
			// æŒ‡æ•°é€€é¿é‡è¯•
			backoffTime := time.Duration(attempt) * time.Second
			logger.Printf("[%s] â³ ç­‰å¾… %v åé‡è¯• [Claude]", requestID, backoffTime)
			time.Sleep(backoffTime)
		}
	}
	
	return &BatchFeedResult{
		Success:    false,
		ChunkIndex: chunk.Index,
		Error:      fmt.Errorf("ç¬¬ %d æ‰¹æ¬¡æŠ•å–‚å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡ [Claude]", chunk.Index, maxRetries),
		RetryCount: maxRetries,
	}
}

// feedSingleBatchClaude æŠ•å–‚å•ä¸ªæ•°æ®å—ï¼ˆClaudeç‰ˆæœ¬ï¼‰
func (s *SilicoIDInterceptor) feedSingleBatchClaude(ctx context.Context, chunk ContentChunk, requestID string, data map[string]interface{}) error {
	// æ„é€ åˆ†æ‰¹æ¶ˆæ¯
	var batchMessage string
	if chunk.Total == 1 {
		// åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ­£å¸¸å¤„ç†
		batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœï¼š\n\n```json\n%s\n```\n\nè¯·åŸºäºä»¥ä¸Šç»“æœç»§ç»­å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚", 
			chunk.ToolName, chunk.Content)
	} else {
		// å¤šä¸ªæ‰¹æ¬¡ï¼Œæ·»åŠ æ‰¹æ¬¡ä¿¡æ¯
		if chunk.IsLast {
			batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœ (ç¬¬ %d/%d æ‰¹æ¬¡ï¼Œæœ€åä¸€æ‰¹)ï¼š\n\n```json\n%s\n```\n\næ‰€æœ‰æ•°æ®å·²æŠ•å–‚å®Œæ¯•ï¼Œè¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ç»“æœç»§ç»­å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚", 
				chunk.ToolName, chunk.Index, chunk.Total, chunk.Content)
		} else {
			batchMessage = fmt.Sprintf("å·¥å…·è°ƒç”¨ '%s' çš„æ‰§è¡Œç»“æœ (ç¬¬ %d/%d æ‰¹æ¬¡)ï¼š\n\n```json\n%s\n```\n\nè¿™æ˜¯ç¬¬ %d æ‰¹æ•°æ®ï¼Œè¯·ç­‰å¾…æ‰€æœ‰æ•°æ®æŠ•å–‚å®Œæ¯•åå†å›ç­”ã€‚", 
				chunk.ToolName, chunk.Index, chunk.Total, chunk.Content, chunk.Index)
		}
	}
	
	// è·å–å½“å‰messages
	messages, _ := data["messages"].([]interface{})
	
	// æ·»åŠ å·¥å…·ç»“æœ
	messages = append(messages, map[string]interface{}{
		"id":      uuid.New().String(),
		"role":    "user",
		"content": batchMessage,
	})
	
	// æ›´æ–°è¯·æ±‚æ•°æ®
	data["messages"] = messages
	
	// å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œè®©AIç¡®è®¤æ¥æ”¶
	if !chunk.IsLast {
		logger.Printf("[%s] â³ ç¬¬ %d æ‰¹æ¬¡å·²æŠ•å–‚ï¼Œç­‰å¾…AIç¡®è®¤æ¥æ”¶ [Claude]", requestID, chunk.Index)
		
		// è½¬æ¢ä¸ºClaudeæ ¼å¼
		claudeData, err := s.formatConverter.RequestOpenAIToClaude(data)
		if err != nil {
			return fmt.Errorf("æ‰¹æ¬¡ç¡®è®¤æ ¼å¼è½¬æ¢å¤±è´¥: %v", err)
		}
		
		// è°ƒç”¨Claude APIç¡®è®¤æ¥æ”¶
		confirmClaudeResponse := s.claudeService.CreateChatCompletionNonStream(ctx, claudeData)
		
		// å°†Claudeå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼
		confirmResponse, err := s.formatConverter.ResponseClaudeToOpenAI(confirmClaudeResponse, data)
		if err != nil {
			return fmt.Errorf("æ‰¹æ¬¡ç¡®è®¤å“åº”è½¬æ¢å¤±è´¥: %v", err)
		}
		
		// æ£€æŸ¥ç¡®è®¤å“åº”æ˜¯å¦æœ‰é”™è¯¯
		if errObj, exists := confirmResponse["error"]; exists {
			return fmt.Errorf("AIç¡®è®¤è°ƒç”¨è¿”å›é”™è¯¯: %v", errObj)
		}
		
		// æå–ç¡®è®¤å“åº”
		if confirmChoices, ok := confirmResponse["choices"].([]interface{}); ok && len(confirmChoices) > 0 {
			if confirmChoice, ok := confirmChoices[0].(map[string]interface{}); ok {
				if confirmMessage, ok := confirmChoice["message"].(map[string]interface{}); ok {
					if confirmContent, ok := confirmMessage["content"].(string); ok {
						logger.Printf("[%s] âœ… ç¬¬ %d æ‰¹æ¬¡ç¡®è®¤å“åº”: %s [Claude]", requestID, chunk.Index, truncateString(confirmContent, 100))
						
						// æ·»åŠ AIçš„ç¡®è®¤å“åº”
						messages = append(messages, map[string]interface{}{
							"id":      uuid.New().String(),
							"role":    "assistant",
							"content": confirmContent,
						})
						
						// æ›´æ–°messages
						data["messages"] = messages
					} else {
						return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”å†…å®¹")
					}
				} else {
					return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”æ¶ˆæ¯")
				}
			} else {
				return fmt.Errorf("æ— æ³•æå–AIç¡®è®¤å“åº”é€‰æ‹©")
			}
		} else {
			return fmt.Errorf("AIç¡®è®¤å“åº”æ ¼å¼æ— æ•ˆ")
		}
	}
	
	return nil
}

// FeedLargeFileChunks å¤„ç†å¤§æ–‡ä»¶åˆ†å—çš„åˆ†æ‰¹æŠ•å–‚
// æ£€æµ‹è¯·æ±‚æ•°æ®ä¸­çš„ _large_file_chunksï¼Œå¦‚æœæœ‰åˆ™åˆ†æ‰¹æŠ•å–‚ç»™AI
func (s *SilicoIDInterceptor) FeedLargeFileChunks(ctx context.Context, requestID string, data map[string]interface{}) error {
	// æ£€æŸ¥æ˜¯å¦æœ‰å¤§æ–‡ä»¶åˆ†å—
	largeFileChunks, ok := data["_large_file_chunks"].([]interface{})
	if !ok || len(largeFileChunks) == 0 {
		return nil // æ²¡æœ‰å¤§æ–‡ä»¶åˆ†å—ï¼Œç›´æ¥è¿”å›
	}
	
	logger.Printf("[%s] ğŸ“¦ æ£€æµ‹åˆ°å¤§æ–‡ä»¶åˆ†å—ï¼Œå…± %d å—ï¼Œå¼€å§‹åˆ†æ‰¹æŠ•å–‚", requestID, len(largeFileChunks))
	
	// åˆ¤æ–­æ˜¯å¦æ˜¯ Claude æ¨¡å‹
	modelName, _ := data["model"].(string)
	isClaudeModel := s.isClaudeModel(modelName)
	
	// å°†åˆ†å—è½¬æ¢ä¸º ContentChunk æ ¼å¼
	chunks := make([]ContentChunk, 0, len(largeFileChunks))
	for _, chunkInterface := range largeFileChunks {
		chunkMap, ok := chunkInterface.(map[string]interface{})
		if !ok {
			continue
		}
		
		fileId, _ := chunkMap["file_id"].(string)
		index, _ := chunkMap["index"].(float64)
		content, _ := chunkMap["content"].(string)
		isLast, _ := chunkMap["is_last"].(bool)
		
		// è®¡ç®—æ€»å—æ•°
		total := len(largeFileChunks)
		
		chunks = append(chunks, ContentChunk{
			Index:    int(index),
			Total:    total,
			Content:  content,
			IsLast:   isLast,
			ToolName: fmt.Sprintf("æ–‡ä»¶å†…å®¹_%s", fileId),
		})
	}
	
	// æŒ‰ç´¢å¼•æ’åº
	for i := 0; i < len(chunks)-1; i++ {
		for j := i + 1; j < len(chunks); j++ {
			if chunks[i].Index > chunks[j].Index {
				chunks[i], chunks[j] = chunks[j], chunks[i]
			}
		}
	}
	
	// åˆ†æ‰¹æŠ•å–‚
	const maxRetries = 3
	feedResults := make([]*BatchFeedResult, 0, len(chunks))
	
	for _, chunk := range chunks {
		var result *BatchFeedResult
		if isClaudeModel {
			result = s.feedBatchWithRetryClaude(ctx, chunk, requestID, data, maxRetries)
		} else {
			result = s.feedBatchWithRetry(ctx, chunk, requestID, data, maxRetries)
		}
		feedResults = append(feedResults, result)
	}
	
	// éªŒè¯æ‰€æœ‰æ‰¹æ¬¡æ˜¯å¦éƒ½æˆåŠŸ
	if err := s.validateAllBatchesFed(feedResults, requestID); err != nil {
		return fmt.Errorf("å¤§æ–‡ä»¶åˆ†æ‰¹æŠ•å–‚å¤±è´¥: %v", err)
	}
	
	logger.Printf("[%s] âœ… å¤§æ–‡ä»¶åˆ†æ‰¹æŠ•å–‚å®Œæˆï¼Œå…± %d å—", requestID, len(chunks))
	
	// æ¸…ç†æ ‡è®°
	delete(data, "_large_file_chunks")
	
	return nil
}
